{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rznSDgbvGggG"
   },
   "source": [
    "## Tic-Tac-Toe Agent\n",
    "â€‹\n",
    "In this notebook, you will learn to build an RL agent (using Q-learning) that learns to play Numerical Tic-Tac-Toe with odd numbers. The environment is playing randomly with the agent, i.e. its strategy is to put an even number randomly in an empty cell. The following is the layout of the notebook:\n",
    "        - Defining epsilon-greedy strategy\n",
    "        - Tracking state-action pairs for convergence\n",
    "        - Define hyperparameters for the Q-learning algorithm\n",
    "        - Generating episode and applying Q-update equation\n",
    "        - Checking convergence in Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eDb8PxBGggH"
   },
   "source": [
    "#### Importing libraries\n",
    "Write the code to import Tic-Tac-Toe class from the environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SFNYceFGggJ"
   },
   "outputs": [],
   "source": [
    "from TCGame_Env_Mukul import TicTacToe # - import your class from environment file\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYLQyopEG8nz"
   },
   "outputs": [],
   "source": [
    "# Function to convert state array into a string to store it as keys in the dictionary\n",
    "# states in Q-dictionary will be of form: x-4-5-3-8-x-x-x-x\n",
    "#   x | 4 | 5\n",
    "#   ----------\n",
    "#   3 | 8 | x\n",
    "#   ----------\n",
    "#   x | x | x\n",
    "\n",
    "def Q_state(state):\n",
    "\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZebMOoiVHBBr"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
    "\n",
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRciPUkYHDWf"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will add new Q-values to the Q-dictionary. \n",
    "\n",
    "def add_to_dict(state):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in Q_dict.keys():\n",
    "        for action in valid_act:\n",
    "            Q_dict[state1][action]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNNi_EfHGggM"
   },
   "source": [
    "#### Epsilon-greedy strategy - Write your code here\n",
    "\n",
    "(you can build your epsilon-decay function similar to the one given at the end of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0lMfqiJGggN"
   },
   "outputs": [],
   "source": [
    "# Defining epsilon-greedy policy.\n",
    "\n",
    "def epsilon_greedy(state, time):\n",
    "    max_epsilon = 1.0\n",
    "        min_epsilon = 0.001\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.000001*time)\n",
    "    \n",
    "    z = np.random.random()       \n",
    "    \n",
    "    if z > epsilon:\n",
    "        action = max(Q_dict[Q_state(state)],key=Q_dict[Q_state(state)].get) # exploitation         \n",
    "    else:\n",
    "        action = random.sample(valid_actions(state),1)[0]   # exploration\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2kyQHOMGggR"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence - write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [],
   "source": [
    "# Initialise Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "\n",
    "Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "States_track = collections.defaultdict(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs73iv8fHOxV"
   },
   "outputs": [],
   "source": [
    "# Initialise few random states to be tracked\n",
    "\n",
    "def initialise_tracking_states():\n",
    "    sample_qs = [('x-x-x-x-6-x-x-x-5',(2,7)),\n",
    "                 ('x-x-x-x-9-x-6-x-x',(1,7)),\n",
    "                 ('x-3-x-x-x-6-x-x-x',(0,1)),\n",
    "                 ('x-5-x-2-x-x-4-7-x',(0,9)),\n",
    "                 ('x-x-7-x-x-x-x-x-2',(1,5)),\n",
    "                 ('5-x-x-x-x-6-x-x-x',(4,9))]\n",
    "    \n",
    "    for q_val in sample_qs:\n",
    "        state = q_val[0]\n",
    "        action = q_val[1]\n",
    "        States_track[state][action] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAbwJDMVHpwl"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Pyj7nMVHsBi"
   },
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_8xSluUHvew"
   },
   "outputs": [],
   "source": [
    "#Initialise tracking states\n",
    "\n",
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iPt--E9GggV"
   },
   "source": [
    "#### Define hyperparameters  ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0_f5czFGggW"
   },
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "# I have used different hyperparamters like Episode = 5m, 5.5m, 5.8m, 6m and 6.2m. LR = 0.1, 0.15, 0.2, 0.22 etc. \n",
    "# Below set are best set of HP\n",
    "\n",
    "LR = 0.22\n",
    "Gamma = 0.75\n",
    "Episode = 6200000\n",
    "threshold = 2500\n",
    "checkpoint_print_episodes = 620000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6twJ7wGggh"
   },
   "source": [
    "### Q-update loop ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldCgQuDNGggj"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "start_time = time.time()\n",
    "\n",
    "# Keys for random state to be tracked\n",
    "track={}\n",
    "track['x-x-x-x-6-x-x-x-5']=[]\n",
    "track['x-x-x-x-9-x-6-x-x']=[]\n",
    "track['x-5-x-2-x-x-4-7-x']=[]\n",
    "track['x-3-x-x-x-6-x-x-x']=[]\n",
    "track['x-x-7-x-x-x-x-x-2']=[]\n",
    "track['5-x-x-x-x-6-x-x-x']=[]\n",
    "\n",
    "# Initalizing count of game won by agent, env and tie\n",
    "agent_won_cnt = 0\n",
    "env_won_cnt = 0\n",
    "tie_cnt = 0\n",
    "\n",
    "for episode in tqdm(range(Episode)):\n",
    "    \n",
    "    env = TicTacToe()  #Calling my env class\n",
    "    \n",
    "    ## Initalizing parameters for the episodes\n",
    "    reward=0\n",
    "    curr_state = env.state\n",
    "    add_to_dict(curr_state)\n",
    "    is_terminal = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not(is_terminal):\n",
    "        curr_action = epsilon_greedy(curr_state, episode) #call epsilon greedy function to get current action\n",
    "    \n",
    "        # if Q_state is in Q state to be track then append the action\n",
    "        if Q_state(curr_state) in track.keys():\n",
    "            track[Q_state(curr_state)].append(curr_action)\n",
    "            \n",
    "        # call step function to get next state, reward, terminal state and final flag\n",
    "        next_state, reward, is_terminal, final_flag = env.step(curr_state,curr_action) \n",
    "        \n",
    "        curr_state_lookup = Q_state(curr_state)\n",
    "        next_state_lookup = Q_state(next_state)\n",
    "\n",
    "        if is_terminal:\n",
    "            q_value_max = 0 # Initalizing Q max\n",
    "            \n",
    "            # Tracking the count of games won by agent and environment\n",
    "            if final_flag == \"A\":\n",
    "                agent_won_cnt += 1\n",
    "            elif final_flag == \"E\":\n",
    "                env_won_cnt += 1\n",
    "            else:\n",
    "                tie_cnt += 1\n",
    "        else:\n",
    "            add_to_dict(next_state)\n",
    "            max_next = max(Q_dict[next_state_lookup],key=Q_dict[next_state_lookup].get)\n",
    "            q_value_max = Q_dict[next_state_lookup][max_next]\n",
    "        \n",
    "        # Q update\n",
    "        Q_dict[curr_state_lookup][curr_action] += LR * ((reward + (Gamma * (q_value_max))) \n",
    "                                                        - Q_dict[curr_state_lookup][curr_action]) \n",
    "        curr_state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "    \n",
    "    # print game state i.e. how many game are tie or won by env or agent\n",
    "    if (episode + 1) % checkpoint_print_episodes == 0:\n",
    "        print(\"After playing %d games, Agent Won : %.3f, Environment Won : %.3f, Tie : %.3f\"% (episode + 1, \n",
    "            agent_won_cnt / (episode + 1), env_won_cnt /(episode + 1), tie_cnt / (episode + 1)))\n",
    "    \n",
    "    # save tracking states\n",
    "    if ((episode + 1) % threshold) == 0:   \n",
    "        save_tracking_states()\n",
    "    \n",
    "    # print when every million completed\n",
    "    if ((episode + 1) % 1000000) == 0:\n",
    "        print('Processed %dM episodes'%((episode+1)/1000000))\n",
    "        \n",
    "elapsed_time = time.time() - start_time\n",
    "save_obj(States_track,'States_tracked')\n",
    "save_obj(Q_dict,'Policy')\n",
    "\n",
    "print('Total Execution time: ', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6eMFbb8Ggg2"
   },
   "source": [
    "#### Check the Q-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr9d2fcVGgg4"
   },
   "outputs": [],
   "source": [
    "#the Q-dictionary\n",
    "\n",
    "Q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1tnDJWkGgg9"
   },
   "outputs": [],
   "source": [
    "# length of the Q-dictionary\n",
    "\n",
    "len(Q_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFgUqfcQGghB"
   },
   "outputs": [],
   "source": [
    "# try checking for one of the states - that which action your agent thinks is the best  -----This will not be evaluated\n",
    "\n",
    "Q_dict['x-5-x-2-x-x-4-7-x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGPZEQDFGghG"
   },
   "source": [
    "#### Check the states tracked for Q-values convergence\n",
    "(non-evaluative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s1Tvz8HGghH"
   },
   "outputs": [],
   "source": [
    "# Write the code for plotting the graphs for state-action pairs tracked\n",
    "\n",
    "plt.figure(0, figsize=(16,7))\n",
    "plt.subplot(241)\n",
    "t1=States_track['x-x-x-x-6-x-x-x-5'][(2,7)]\n",
    "plt.title(\"state-action-1\")\n",
    "plt.plot(np.asarray(range(0, len(t1))),np.asarray(t1))\n",
    "\n",
    "plt.subplot(242)\n",
    "t2=States_track['x-x-x-x-9-x-6-x-x'][(1,7)]\n",
    "plt.title(\"state-action-2\")\n",
    "plt.plot(np.asarray(range(0, len(t2))),np.asarray(t2))\n",
    "\n",
    "plt.subplot(243)\n",
    "t3=States_track['x-5-x-2-x-x-4-7-x'][(0,9)]\n",
    "plt.title(\"state-action-3\")\n",
    "plt.plot(np.asarray(range(0, len(t3))),np.asarray(t3))\n",
    "\n",
    "plt.subplot(244)\n",
    "t4=States_track['x-x-7-x-x-x-x-x-2'][(1,5)]\n",
    "plt.title(\"state-action-4\")\n",
    "plt.plot(np.asarray(range(0, len(t4))),np.asarray(t4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2Opp8_NITkC"
   },
   "source": [
    "### Epsilon - decay check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQ_D_JsuGghR"
   },
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "time = np.arange(0,6200000)\n",
    "epsilon = []\n",
    "for i in range(0,6200000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.000001*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "J7c2xADQGghV",
    "outputId": "cb60fce3-570b-45fb-bd83-abde3d13b273"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59BRf43IJiQ1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_Agent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
